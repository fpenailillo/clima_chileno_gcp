# Sistema de IntegraciÃ³n con Google Weather API en GCP

Sistema serverless event-driven para la extracciÃ³n, procesamiento y almacenamiento de datos climÃ¡ticos de ubicaciones en Chile utilizando la Google Weather API y servicios de Google Cloud Platform.

## DescripciÃ³n

Este proyecto implementa una arquitectura moderna de datos meteorolÃ³gicos basada en eventos que:

- **Extrae** datos climÃ¡ticos de la Google Weather API para ubicaciones configuradas en Chile
- **Procesa** los datos de forma asÃ­ncrona usando Pub/Sub como bus de mensajes
- **Almacena** los datos en una arquitectura medallion:
  - **Capa Bronce** (Cloud Storage): Datos crudos sin transformar
  - **Capa Plata** (BigQuery): Datos limpios y estructurados para anÃ¡lisis
- **Orquesta** la extracciÃ³n periÃ³dica mediante Cloud Scheduler

## Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Cloud Scheduler â”‚ (Cada hora)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Cloud Function: Extractor   â”‚
â”‚ â€¢ Llama a Weather API       â”‚
â”‚ â€¢ OAuth 2.0 authentication  â”‚
â”‚ â€¢ Publica a Pub/Sub         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Pub/Sub Topic               â”‚
â”‚ â€¢ clima-datos-crudos        â”‚
â”‚ â€¢ Dead Letter Queue         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Cloud Function: Procesador  â”‚
â”‚ â€¢ Procesa mensajes Pub/Sub  â”‚
â”‚ â€¢ Valida y transforma datos â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Cloud Storage    â”‚   â”‚ BigQuery         â”‚
â”‚ (Capa Bronce)    â”‚   â”‚ (Capa Plata)     â”‚
â”‚ â€¢ Datos crudos   â”‚   â”‚ â€¢ Datos          â”‚
â”‚ â€¢ Particionado   â”‚   â”‚   estructurados  â”‚
â”‚   por fecha      â”‚   â”‚ â€¢ Particionado   â”‚
â”‚ â€¢ Versionado     â”‚   â”‚ â€¢ Clustering     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Inicio RÃ¡pido

Para desplegar el sistema completo en tu proyecto GCP `clima-chileno`:

```bash
# 1. Configurar proyecto
gcloud config set project clima-chileno

# 2. Autenticarse
gcloud auth login
gcloud auth application-default login

# 3. Desplegar infraestructura completa
./desplegar.sh clima-chileno us-central1
```

**Â¡Listo!** El sistema comenzarÃ¡ a extraer datos climÃ¡ticos automÃ¡ticamente cada hora.

### Verificar el despliegue

```bash
# Ver logs del extractor
gcloud functions logs read extractor-clima --gen2 --region=us-central1 --limit=20

# Consultar datos en BigQuery
bq query --use_legacy_sql=false \
  'SELECT nombre_ubicacion, temperatura, descripcion_clima, hora_actual
   FROM clima.condiciones_actuales
   ORDER BY hora_actual DESC
   LIMIT 5'
```

## Ubicaciones Monitoreadas

El sistema monitorea las siguientes ubicaciones en Chile:

| UbicaciÃ³n | Latitud | Longitud | DescripciÃ³n |
|-----------|---------|----------|-------------|
| **Santiago** | -33.4489 | -70.6693 | Capital de Chile |
| **Farellones** | -33.3558 | -70.2989 | Centro de esquÃ­ en la cordillera |
| **ValparaÃ­so** | -33.0472 | -71.6127 | Puerto principal |

## CaracterÃ­sticas TÃ©cnicas

### Cloud Function: Extractor

- **Trigger**: HTTP (invocado por Cloud Scheduler)
- **Runtime**: Python 3.11
- **Memoria**: 256 MB
- **Timeout**: 60 segundos
- **Funcionalidades**:
  - AutenticaciÃ³n OAuth 2.0 con Google Weather API
  - Llamadas paralelas para mÃºltiples ubicaciones
  - Enriquecimiento de datos con metadata
  - PublicaciÃ³n a Pub/Sub con atributos para routing
  - Manejo robusto de errores y logging estructurado

### Cloud Function: Procesador

- **Trigger**: Pub/Sub (topic: clima-datos-crudos)
- **Runtime**: Python 3.11
- **Memoria**: 512 MB
- **Timeout**: 120 segundos
- **Funcionalidades**:
  - DecodificaciÃ³n y validaciÃ³n de mensajes
  - Almacenamiento de datos crudos en Cloud Storage
  - TransformaciÃ³n a esquema estructurado
  - InserciÃ³n en BigQuery
  - Reintentos automÃ¡ticos con exponential backoff
  - Dead letter queue para mensajes fallidos

### Cloud Storage (Capa Bronce)

- **Estructura de particiones**: `{ubicacion}/{AAAA}/{MM}/{DD}/{timestamp}.json`
- **Versionado**: Habilitado
- **Ciclo de vida**:
  - 0-30 dÃ­as: Standard
  - 30-90 dÃ­as: Nearline
  - 90-365 dÃ­as: Coldline
  - 365+ dÃ­as: EliminaciÃ³n automÃ¡tica

### BigQuery (Capa Plata)

- **Dataset**: `clima`
- **Tabla**: `condiciones_actuales`
- **Particionamiento**: Por `DATE(hora_actual)`
- **Clustering**: Por `nombre_ubicacion`
- **Esquema** (27 campos):
  - IdentificaciÃ³n: ubicaciÃ³n, coordenadas
  - Temporal: hora, zona horaria
  - Temperatura: actual, sensaciÃ³n tÃ©rmica, punto de rocÃ­o, Ã­ndice de calor
  - Condiciones: descripciÃ³n, cÃ³digo
  - PrecipitaciÃ³n: probabilidad, acumulaciÃ³n
  - Viento: velocidad, direcciÃ³n, sensaciÃ³n de viento
  - AtmosfÃ©ricas: presiÃ³n, humedad, visibilidad
  - Otras: Ã­ndice UV, cobertura de nubes, probabilidad de tormenta
  - Metadata: timestamp de ingesta, URI datos crudos, JSON completo

## Requisitos Previos

### Software Necesario

- **Google Cloud SDK** (gcloud CLI) versiÃ³n 400+
- **Python** 3.11+
- **Git** para control de versiones

### Cuenta de Google Cloud

1. Proyecto de GCP activo
2. FacturaciÃ³n habilitada
3. Permisos necesarios:
   - Editor de proyecto o roles especÃ­ficos:
     - Cloud Functions Admin
     - Pub/Sub Admin
     - Storage Admin
     - BigQuery Admin
     - Service Account Admin
     - Cloud Scheduler Admin

### APIs Requeridas

Las siguientes APIs deben estar habilitadas (el script de despliegue las habilita automÃ¡ticamente):

- Cloud Functions API
- Cloud Build API
- Cloud Scheduler API
- Pub/Sub API
- Cloud Storage API
- BigQuery API
- Cloud Logging API
- Cloud Run API

## ConfiguraciÃ³n

### 1. Clonar el Repositorio

```bash
git clone <url-repositorio>
cd clima_chileno_gcp
```

### 2. Configurar Variables de Entorno

```bash
export ID_PROYECTO="clima-chileno"
export REGION="us-central1"
```

### 3. AutenticaciÃ³n con GCP

```bash
# Autenticarse con cuenta de GCP
gcloud auth login

# Configurar proyecto
gcloud config set project $ID_PROYECTO

# Configurar credenciales para Application Default Credentials
gcloud auth application-default login
```

## Despliegue

### OpciÃ³n 1: Script Automatizado (Recomendado)

El script `desplegar.sh` despliega toda la infraestructura automÃ¡ticamente:

```bash
./desplegar.sh [ID_PROYECTO] [REGION]
```

Ejemplo:

```bash
./desplegar.sh clima-chileno us-central1
```

El script realiza las siguientes acciones:

1. âœ“ Valida dependencias
2. âœ“ Habilita APIs necesarias
3. âœ“ Crea cuenta de servicio y asigna permisos
4. âœ“ Crea topics de Pub/Sub (principal y DLQ)
5. âœ“ Crea bucket de Cloud Storage con ciclo de vida
6. âœ“ Crea dataset y tabla de BigQuery
7. âœ“ Despliega Cloud Function Extractor
8. âœ“ Despliega Cloud Function Procesador
9. âœ“ Configura Cloud Scheduler (ejecuciÃ³n cada hora)

**Tiempo estimado**: 5-10 minutos

### OpciÃ³n 2: Despliegue Manual

#### 2.1 Crear Topics de Pub/Sub

```bash
gcloud pubsub topics create clima-datos-crudos --project=$ID_PROYECTO
gcloud pubsub topics create clima-datos-dlq --project=$ID_PROYECTO
```

#### 2.2 Crear Bucket de Cloud Storage

```bash
gsutil mb -p $ID_PROYECTO -l $REGION gs://${ID_PROYECTO}-datos-clima-bronce
gsutil versioning set on gs://${ID_PROYECTO}-datos-clima-bronce
```

#### 2.3 Crear Dataset y Tabla de BigQuery

```bash
# Crear dataset
bq mk --project_id=$ID_PROYECTO --location=$REGION clima

# Crear tabla (el schema completo se crea automÃ¡ticamente con desplegar.sh)
# AquÃ­ se muestra solo la estructura bÃ¡sica
bq mk --project_id=$ID_PROYECTO \
  --table \
  --time_partitioning_field=hora_actual \
  --time_partitioning_type=DAY \
  --clustering_fields=nombre_ubicacion \
  clima.condiciones_actuales
```

#### 2.4 Desplegar Cloud Functions

```bash
# Extractor
gcloud functions deploy extractor-clima \
  --gen2 \
  --runtime=python311 \
  --region=$REGION \
  --source=./extractor \
  --entry-point=extraer_clima \
  --trigger-http \
  --set-env-vars=GCP_PROJECT=$ID_PROYECTO

# Procesador
gcloud functions deploy procesador-clima \
  --gen2 \
  --runtime=python311 \
  --region=$REGION \
  --source=./procesador \
  --entry-point=procesar_clima \
  --trigger-topic=clima-datos-crudos \
  --set-env-vars=GCP_PROJECT=$ID_PROYECTO
```

#### 2.5 Configurar Cloud Scheduler

```bash
# Obtener URL del extractor
URL_EXTRACTOR=$(gcloud functions describe extractor-clima \
  --gen2 --region=$REGION --format='value(serviceConfig.uri)')

# Crear job
gcloud scheduler jobs create http extraer-clima-job \
  --location=$REGION \
  --schedule="0 * * * *" \
  --uri=$URL_EXTRACTOR \
  --http-method=POST
```

## Uso

### EjecuciÃ³n Manual

Para probar el sistema manualmente:

```bash
# Obtener URL del extractor
URL_EXTRACTOR=$(gcloud functions describe extractor-clima \
  --gen2 --region=$REGION --format='value(serviceConfig.uri)')

# Ejecutar extractor
curl -X POST $URL_EXTRACTOR -H "Authorization: Bearer $(gcloud auth print-identity-token)"
```

### EjecuciÃ³n Programada

Cloud Scheduler ejecuta automÃ¡ticamente el extractor cada hora segÃºn la configuraciÃ³n:

- **Frecuencia**: `0 * * * *` (cada hora en punto)
- **Zona horaria**: America/Santiago
- **Reintentos**: Hasta 3 intentos con backoff exponencial

### Ver Logs

```bash
# Logs del extractor
gcloud functions logs read extractor-clima --gen2 --region=$REGION --limit=50

# Logs del procesador
gcloud functions logs read procesador-clima --gen2 --region=$REGION --limit=50

# Logs en tiempo real
gcloud functions logs read extractor-clima --gen2 --region=$REGION --tail
```

### Consultar Datos en BigQuery

#### Ãšltimas 10 mediciones

```sql
SELECT
  nombre_ubicacion,
  hora_actual,
  temperatura,
  descripcion_clima,
  humedad_relativa,
  velocidad_viento
FROM
  `clima.condiciones_actuales`
ORDER BY
  hora_actual DESC
LIMIT 10;
```

#### Promedio de temperatura por ubicaciÃ³n (Ãºltimas 24 horas)

```sql
SELECT
  nombre_ubicacion,
  AVG(temperatura) AS temperatura_promedio,
  MIN(temperatura) AS temperatura_minima,
  MAX(temperatura) AS temperatura_maxima,
  COUNT(*) AS total_mediciones
FROM
  `clima.condiciones_actuales`
WHERE
  hora_actual >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 24 HOUR)
GROUP BY
  nombre_ubicacion
ORDER BY
  nombre_ubicacion;
```

#### Condiciones climÃ¡ticas mÃ¡s frecuentes

```sql
SELECT
  nombre_ubicacion,
  descripcion_clima,
  COUNT(*) AS frecuencia
FROM
  `clima.condiciones_actuales`
WHERE
  descripcion_clima IS NOT NULL
  AND hora_actual >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 7 DAY)
GROUP BY
  nombre_ubicacion,
  descripcion_clima
ORDER BY
  nombre_ubicacion,
  frecuencia DESC;
```

### Explorar Datos Crudos en Cloud Storage

```bash
# Listar archivos recientes
gsutil ls -l gs://${ID_PROYECTO}-datos-clima-bronce/santiago/$(date +%Y)

# Descargar un archivo
gsutil cp gs://${ID_PROYECTO}-datos-clima-bronce/santiago/2024/01/15/20240115_120000.json .

# Ver contenido
cat 20240115_120000.json | jq .
```

## Monitoreo y Alertas

### MÃ©tricas Importantes

1. **Cloud Functions**:
   - Tasa de invocaciones
   - Tasa de errores
   - DuraciÃ³n de ejecuciÃ³n
   - Memoria utilizada

2. **Pub/Sub**:
   - Mensajes publicados/procesados
   - Mensajes no confirmados
   - Mensajes en DLQ

3. **BigQuery**:
   - Filas insertadas
   - Bytes procesados
   - Errores de inserciÃ³n

### Configurar Alertas

Crear alertas en Cloud Monitoring para:

```bash
# Tasa de errores alta en Cloud Functions (>5%)
# Mensajes acumulados en DLQ (>10)
# Falta de datos nuevos en BigQuery (>2 horas sin inserts)
```

Ver [documentaciÃ³n de Cloud Monitoring](https://cloud.google.com/monitoring/docs) para configuraciÃ³n detallada.

## Costos Estimados

EstimaciÃ³n mensual para ejecuciÃ³n cada hora (730 invocaciones/mes):

| Servicio | Uso | Costo Estimado (USD) |
|----------|-----|----------------------|
| Cloud Functions | 1,460 invocaciones | $0.05 |
| Pub/Sub | ~3K mensajes | $0.01 |
| Cloud Storage | 100 GB (bronce) | $2.00 |
| BigQuery | 10 GB almacenado | $0.20 |
| BigQuery | 1 GB queries | $0.01 |
| Cloud Scheduler | 1 job | $0.10 |
| **TOTAL** | | **~$2.37/mes** |

**Nota**: Los costos son aproximados y pueden variar segÃºn el uso real y la regiÃ³n.

## Estructura del Proyecto

```
clima_chileno_gcp/
â”œâ”€â”€ extractor/
â”‚   â”œâ”€â”€ main.py                 # Cloud Function de extracciÃ³n
â”‚   â”œâ”€â”€ requirements.txt        # Dependencias del extractor
â”‚   â””â”€â”€ .gcloudignore          # Archivos a ignorar en deploy del extractor
â”œâ”€â”€ procesador/
â”‚   â”œâ”€â”€ main.py                 # Cloud Function de procesamiento
â”‚   â”œâ”€â”€ requirements.txt        # Dependencias del procesador
â”‚   â””â”€â”€ .gcloudignore          # Archivos a ignorar en deploy del procesador
â”œâ”€â”€ desplegar.sh                # Script de despliegue automatizado (ejecutar esto)
â”œâ”€â”€ .gcloudignore              # Archivos a ignorar en deploy general
â”œâ”€â”€ .gitignore                  # Archivos a ignorar en git
â””â”€â”€ README.md                   # Este archivo (documentaciÃ³n completa)
```

## SoluciÃ³n de Problemas

### Error: Permisos insuficientes

**SÃ­ntoma**: Error 403 o "Permission denied"

**SoluciÃ³n**:
```bash
# Verificar permisos de la cuenta de servicio
gcloud projects get-iam-policy $ID_PROYECTO \
  --flatten="bindings[].members" \
  --filter="bindings.members:serviceAccount:funciones-clima-sa@"

# Asignar permisos faltantes
gcloud projects add-iam-policy-binding $ID_PROYECTO \
  --member="serviceAccount:funciones-clima-sa@${ID_PROYECTO}.iam.gserviceaccount.com" \
  --role="roles/pubsub.publisher"
```

### Error: API no habilitada

**SÃ­ntoma**: "API [nombre] has not been used in project"

**SoluciÃ³n**:
```bash
# Habilitar API especÃ­fica
gcloud services enable [nombre-api].googleapis.com --project=$ID_PROYECTO

# Ejemplo
gcloud services enable cloudfunctions.googleapis.com --project=$ID_PROYECTO
```

### Error: Timeout en Cloud Function

**SÃ­ntoma**: Function timeout, exceeded time limit

**SoluciÃ³n**:
```bash
# Aumentar timeout del extractor
gcloud functions deploy extractor-clima \
  --gen2 \
  --timeout=120s \
  --region=$REGION

# Aumentar timeout del procesador
gcloud functions deploy procesador-clima \
  --gen2 \
  --timeout=180s \
  --region=$REGION
```

### Mensajes en Dead Letter Queue

**SÃ­ntoma**: Mensajes acumulados en topic clima-datos-dlq

**SoluciÃ³n**:
```bash
# Ver mensajes en DLQ
gcloud pubsub subscriptions pull clima-datos-dlq-sub --limit=10

# Revisar logs del procesador para identificar causa
gcloud functions logs read procesador-clima --gen2 --region=$REGION --limit=100

# Reprocesar mensajes manualmente si es necesario
```

## Mejoras Futuras

- [ ] Agregar mÃ¡s ubicaciones monitoreadas
- [ ] Implementar pronÃ³stico extendido (15 dÃ­as)
- [ ] Crear dashboard en Looker Studio o Data Studio
- [ ] AÃ±adir alertas automÃ¡ticas por condiciones climÃ¡ticas extremas
- [ ] Implementar API REST para consultar datos histÃ³ricos
- [ ] Agregar tests unitarios y de integraciÃ³n
- [ ] Implementar CI/CD con Cloud Build
- [ ] Crear capa Gold en BigQuery con agregaciones pre-calculadas
- [ ] AÃ±adir soporte para mÃºltiples paÃ­ses
- [ ] Implementar cachÃ© de datos con Cloud Memorystore

## EstÃ¡ndares de CÃ³digo

Este proyecto sigue estÃ¡ndares estrictos de cÃ³digo en espaÃ±ol:

- **Variables**: `temperatura_actual`, `datos_meteorologicos`
- **Funciones**: `extraer_clima()`, `procesar_mensaje()`, `guardar_datos()`
- **Clases**: `ConfiguracionClima`, `DatosMeteorologicos`
- **Constantes**: `UBICACIONES_MONITOREO`, `ID_PROYECTO`
- **DocumentaciÃ³n**: Docstrings completos en espaÃ±ol con Args, Returns, Raises
- **Comentarios**: Explicaciones en espaÃ±ol
- **Logs**: Mensajes en espaÃ±ol

## Contribuir

1. Fork el proyecto
2. Crear rama de feature (`git checkout -b feature/nueva-funcionalidad`)
3. Commit cambios (`git commit -m 'Agregar nueva funcionalidad'`)
4. Push a la rama (`git push origin feature/nueva-funcionalidad`)
5. Crear Pull Request

## Licencia

Este proyecto estÃ¡ bajo la licencia MIT. Ver archivo `LICENSE` para mÃ¡s detalles.

## Contacto

Para preguntas, problemas o sugerencias, por favor abrir un issue en el repositorio.

## Referencias

- [Google Weather API Documentation](https://developers.google.com/maps/documentation/weather)
- [Google Cloud Functions](https://cloud.google.com/functions/docs)
- [Google Cloud Pub/Sub](https://cloud.google.com/pubsub/docs)
- [Google BigQuery](https://cloud.google.com/bigquery/docs)
- [Cloud Scheduler](https://cloud.google.com/scheduler/docs)
- [Arquitectura Medallion](https://www.databricks.com/glossary/medallion-architecture)

---

**Nota**: Este sistema estÃ¡ diseÃ±ado para propÃ³sitos educativos y de demostraciÃ³n. Para uso en producciÃ³n, considere implementar autenticaciÃ³n mÃ¡s robusta, monitoreo avanzado, y pruebas exhaustivas.
